\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Introduction to High Performance Computing}
\author{Maximilian Georg Kurzawski}
\date{27.01.2023}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{verbatimbox}
\begin{document}

\maketitle

\section{Exercise 1 - List, Tuples, array, and NumPy}

\subsection{What is the advantage of using Lists vs. Tuples}

Lists are dynamic arrays that let us modify and resize the data. 
Tuples on the other hand are static arrays whose contents are fixed and immutable. 
If you append to a list, it will allocate a new area in the memory, and move the original list there, it performs in O(1). 
You can also append to a tuple, this operation is performed as a concatenation of tuples and it performs with O(n). 
If you want to modify a tuple, you have to create a new tuple with the modified values.

\subsection{What is the advantage of using the array module vs. Python lists?}

The array module is a container that can hold a fixed number of items, and these items must all be of the same type. 
It is more efficient in terms of memory usage and performance than lists. 
The array module is more efficient because it is implemented in C, and it uses a contiguous block of memory to store the items. 
The items are stored one after the other, so the array module knows the location of each item, and it can access them directly. 
This is why the array module is more efficient than lists. 
The array module is also more efficient than lists because it can only store items of the same type. 
The array module uses the same amount of memory for each item, so it can store more items than lists, which store each item with a different amount of memory.


\subsection{What are the memory fragmentation problem and Von-Neumann bottleneck? How do they affect the performance of a code? How can we try to address it?}

The memory fragmentation problem is the problem of the memory being fragmented into small pieces, and the memory manager is not able to find a contiguous block of memory large enough to store the data. 
The Von-Neumann bottleneck is the problem of the CPU not being able to fetch the data from the memory fast enough. 
This problem is caused by the fact that the CPU and the memory are connected by a bus, and the bus has a limited bandwidth. 
This problem affects the performance of a code by causing the CPU to wait for the data to be fetched from the memory. 
We can try to address this problem by increasing the bus bandwidth, or by increasing the amount of memory. 
We can also try to decrease the amount of memory fragmentation by using a memory manager that is able to find contiguous blocks of memory, and by using a memory allocator that is able to defragment the memory.


\subsection{What is a page fault? What is the difference between a minor and a major page fault?}

A page fault is an exception that occurs when the CPU tries to access a memory page that is not in the memory.
A minor page fault occurs when the CPU tries to access a memory page that is not in the memory, but the memory manager is able to find a free page in the memory to store the data. 
In contrast the major page fault occurs when the memory manager is not able to find a free page in the memory to store the data.
When a major page fault occurs, the memory manager has to evict a page from the memory, and then it can store the data in the memory.


\subsection{What is the impact of a cache miss on the performance?}

A cache miss occurs when the CPU tries to access a memory page that is not in the cache. 
When a cache miss occurs, the CPU has to wait for the data to be fetched from the memory, or even from the disc. 
This affects the performance of the code because the CPU has to wait for the data to be fetched from the memory. 
This is why we should try to minimize the number of cache misses.


\subsection{Which HPC libraries does your NumPy installation use? Hint: you can check by writing a simple code.}
The HPC libraries that my NumPy installation uses are:

\begin{itemize}
    \item openblas64\_\_info
    \item blas\_ilp64\_opt\_info
    \item openblas64\_\_lapack\_info
    \item lapack\_ilp64\_opt\_info
\end{itemize}
These were the HPC libraries that my NumPy installation uses. 
To check this, I used the following code:
\begin{lstlisting}
    import numpy as np
    print(np.show_config())
\end{lstlisting}


\section{Exercise 2 - STREAM Benchmark in Python to Measure the Memory Bandwidth}

% display all 3 pictures
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{1000000.png}
\caption{STREAM Benchmark Results - 1 Million}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{10000000.png}
\caption{STREAM Benchmark Results - 10 Million}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{100000000.png}
\caption{STREAM Benchmark Results - 100 Million}
\end{figure}

%Answer to the questions
%  How does the bandwidth vary when increasing the STREAM_ARRAY_SIZE and why?
The bandwidth varies when increasing the STREAM\_ARRAY\_SIZE because the bandwidth is proportional to the size of the array.
For all of the results: the bandwidth of the Python list is the lowest, the bandwidth of the Array is the second lowest, and the bandwidth of the NumPy array is the highest.

\section{Exercise 3 - PyTest with the Julia Set Code}

This is the test code for the Julia Set code


\begin{minipage}{\linewidth}
\begin{lstlisting}
import pytest
@pytest.mark.parametrize("desired_width,max_iterations",
                [(1000, 300), (1000, 1000),
                (10000, 300), (100, 100)])
def test_julia_set(desired_width, max_iterations):
    # In order to achieve this we returned the output variable.
    import JuliaSet as js
    output = js.calc_pure_python(desired_width, max_iterations)
    assert sum(output) == 33219980
\end{lstlisting}
\end{minipage}
As you can see for 3.2 we used the pytest.mark.parametrize decorator to test the Julia Set code with different parameters.


\section{Exercise 4 - Python DGEMM Benchmark Operation}

\subsubsection{For which kind of problems do you use the BLAS libraries?}

One uses the BLAS libraries for problems that involve matrix multiplication, and for problems that involve linear algebra.
BLAS libraries are highly tuned for performance. 
They are also easy to use.

\subsubsection{What is the difference between BLAS level-1, level-2 and level-3?}

BLAS level-1 is the simplest level of BLAS.
It is used for vector-vector operations.
BLAS level-2 is used for matrix-vector operations.
BLAS level-3 is used for matrix-matrix operations.


% dgemm double-precision dense matrix-matrix multiplication

\subsection{Measure the execution time for each approach varying the size of the matrix. Report the average and error (std. deviation or min/max or interval of confidence). Answer the question: how the computational performance varies with increasing the size of the matrices and why so?}

Here are the results for the DGEMM benchmark operation:
\begin{lstlisting}
    Average time for array size:  50
    Numpy average time:  0.019524256388346355
    List average time:  0.019973119099934895
    Array average time:  0.01978898048400879
    Numpy standard deviation:  0.0161988019210983
    List standard deviation:  0.01712826601838285
    Array standard deviation:  0.016751529326259916
    Average time for array size:  100
    Numpy average time:  0.23694856961568198
    List average time:  0.17482391993204752
    Array average time:  0.17079544067382812
    Numpy standard deviation:  0.19394822375290166
    List standard deviation:  0.13921506985460658
    Array standard deviation:  0.13796664580452822
    Average time for array size:  150
    Numpy average time:  0.5743474165598551
    List average time:  0.5775994459788004
    Array average time:  0.5648147265116373
    Numpy standard deviation:  0.47600947175501684
    List standard deviation:  0.49428629664933804
    Array standard deviation:  0.4662991817964516
    Average time for array size:  200
    Numpy average time:  1.364508867263794
    List average time:  1.4190724690755208
    Array average time:  1.3160467147827148
    Numpy standard deviation:  1.1521688895650994
    List standard deviation:  1.2162573643440988
    Array standard deviation:  1.1110624743584603
\end{lstlisting}

% how the computational performance varies with increasing the size of the matrices and why so?
The computational performance varies with increasing the size of the matrices because the computational performance is proportional to the size of the matrices.
The computational performance of the NumPy array is the highest, the computational performance of the Array is the second highest, and the computational performance of the Python list is the lowest.

% calculate Flops/s
\subsubsection{Calculate the Flops/s for each approach.}

The ammount of FLOPs can be calculated for each array size using the following formula:
\begin{equation}
    FLOPS = 2 * N^3
\end{equation}
where N is the size of the array.
This results in these FLOPS values:
\begin{itemize}
    \item array size:  50, FLOPS:  125000
    \item array size: 100, FLOPS:  2000000
    \item array size: 150, FLOPS:  3375000
    \item array size: 200, FLOPS:  6400000
\end{itemize}
To calculate the Flops/s for each approach, we can use the following formula:
\begin{equation}
    Flops/s = FLOPS / time
\end{equation}
This results in these Flops/s values:
For Approach using numpy:
\begin{itemize}
    \item array size:  50, Flops/s:  6.452213581502256e+06
    \item array size: 100, Flops/s:  8.456655341569153e+06
    \item array size: 150, Flops/s:  5.86158766211901e+06
    \item array size: 200, Flops/s:  4.700520412733408e+06
\end{itemize}
For Approach using lists:
\begin{itemize}
    \item array size:  50, Flops/s:  6.279273737621226e+06
    \item array size: 100, Flops/s:  11.460815920013299e+06
    \item array size: 150, Flops/s:  5.831359680297444e+06
    \item array size: 200, Flops/s:  4.5058934535707475e+06
\end{itemize}

For Approach using arrays:
\begin{itemize}
    \item array size:  50, Flops/s:  6.316115830078326e+06
    \item array size: 100, Flops/s:  11.703580386807087e+06
    \item array size: 150, Flops/s:  5.961116375030808e+06
    \item array size: 200, Flops/s:  4.864849510306772e+06
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Exercise4_plot.png}
    \caption{Numpy}
    \label{fig:numpy}
\end{figure}

\subsubsection{How do the FLOPS/s you measured compares to the theoretical peak of your processor (if we assume that we do one operation per cycle, then the peak is the clock frequency value)}

If we divide the theoretical peak of the processor by the calculated value we are getting a percentage that indicates the processors utilisation.
A lower percentage indicates that the processor is not being utilized efficiently, whereas a higher percentage indicates that the processor is being utilized efficiently.
Theoretical peak FLOPS are often not attainable in real-world applications due to various factors such as memory access times, branching, and parallelism. 



\section{Exercise 5 - A Python Discrete Fourier Transform}

\subsection{Develop a DFT in Python and a unit test with pytest to check the calculation's correctness. Also, use the Python logging module to log the results. The data structures (lists, array, or NumPy) are of your choice.}

This is the essential part of the dft function:


\begin{minipage}{\linewidth}
\begin{verbatim}
    for k in range(n_n):
        for j in range(n_n):
            xr_o[k] += x_r[j] * \
                math.cos(j * k * pi_2 / n_n) + x_i[j] \
                * math.sin(j * k * pi_2 / n_n)
            xi_o[k] += -x_r[j] * \
                math.sin(j * k * pi_2 / n_n) + x_i[j] \
                * math.cos(j * k * pi_2 / n_n)
\end{verbatim}
\end{minipage}

And the following is the according test:


\begin{minipage}{\linewidth}
\begin{lstlisting}
    xr = [1, 2, 3, 4]
    xi = [0, 0, 0, 0]
    N = len(xr)
    Xr_o = [0] * N
    Xi_o = [0] * N
    dft.dft(xr, xi, Xr_o, Xi_o, N)
    expected_Xr_o = [10, -2, -2, -2]
    expected_Xi_o = [0.0, 2, 0, -2]
    for i in range(N):
        assert math.isclose(Xr_o[i], 
                            expected_Xr_o[i], 
                            abs_tol=1e-6)
        assert math.isclose(Xi_o[i], 
                            expected_Xi_o[i], 
                            abs_tol=1e-6)
\end{lstlisting}
\end{minipage}


And for the logging:


\begin{minipage}{\linewidth}
\begin{lstlisting}
mk@DESKTOP-QOQRB5F:~/HPC/Task2/DFT$ python dft.py
INFO:root:Calculating DFT of [1, 2, 3, 4], [0, 0, 0, 0]
INFO:root:Result is [10.0, -2.0000000000000004, 
                    -2.0, -1.9999999999999982], 
                    [0.0, 1.9999999999999996, 
                    -9.797174393178826e-16, 
                    -2.000000000000001]
\end{lstlisting}
\end{minipage}

\subsection{Task 5.2 Document the code using docstrings and generate automatic HTML documentation.}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{sphinx_screen.PNG}
\caption{Sphinx screenshot}
\end{figure}

\subsection{Task 5.3 Check your code using a Python linter. Address the issues raised by the linter, including the style issues. To address some of the issues, you can use a Python auto-formatter.}



\begin{minipage}{\linewidth}
\begin{lstlisting}
mk@DESKTOP-QOQRB5F:~/HPC/Task2/DFT$ pylint dft.py
--------------------------------------------------------------------
Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00)
\end{lstlisting}
\end{minipage}

\subsection{Task 5.4 Measure the execution time, varying the input size from 8 to 1024 elements, and plot it.}

\begin{figure}[h]
\centering
\begin{minipage}[c]{\textwidth}
\includegraphics[width=0.8\textwidth]{dft_time.png}
\caption{DFT Times}
\end{minipage}
\end{figure}


\subsection{Task 5.5 Profiler}

The first profiler I thought would be good to run was cProfile. It is always good to know how many calls are made to which function and how much time these need.
This is the result (I will be showing the first 8 functions):
\begin{minipage}{\linewidth}
\begin{verbatim}
    

         4279269 function calls (4276860 primitive calls) in 2.735 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    138/1    0.001    0.000    2.735    2.735 {built-in method builtins.exec}
        1    0.000    0.000    2.735    2.735 dft.py:1(<module>)
        1    0.000    0.000    2.648    2.648 dft.py:40(main)
        1    0.000    0.000    2.648    2.648 dft.py:36(test_DFT)
        2    2.293    1.146    2.648    1.324 dft.py:12(dft)
  2097184    0.179    0.000    0.179    0.000 {built-in method math.cos}
  2097184    0.176    0.000    0.176    0.000 {built-in method math.sin}
       13    0.001    0.000    0.172    0.013 __init__.py:1(<module>)
\end{verbatim}
\end{minipage}


Now to get a more indepth understanding on the seperate lines and their "complexity", I went ahead and used the line\_profiler to get line by line profiling results


\begin{minipage}{\linewidth}
\begin{verbatim}
    26      1024        149.3      0.1      0.0      for k in range(n_n):
    27   1048576     160607.8      0.2      4.7          for j in range(n_n):
    28   2097152     569190.1      0.3     16.7              xr_o[k] += x_r[j] * \
    29   2097152    1027032.7      0.5     30.1                  math.cos(j * k * pi_2 / n_n) + x_i[j] * math.sin(j * k * pi_2 / n_n)
    30   2097152     621879.7      0.3     18.3              xi_o[k] += -x_r[j] * \
    31   2097152    1027778.9      0.5     30.2                  math.sin(j * k * pi_2 / n_n) + x_i[j] * math.cos(j * k * pi_2 / n_n)
\end{verbatim}
\end{minipage}


As with the first assignment, I would love to check the results provided by the memory profiler. But this is my output.

\begin{minipage}{\linewidth}
\begin{verbatim}
mk@DESKTOP-QOQRB5F:~/HPC/Task2/DFT$ python -m memory_profiler dft.py
mk@DESKTOP-QOQRB5F:~/HPC/Task2/DFT$ 
\end{verbatim}
\end{minipage}


I was however able to perform it on another system.
Here is the output:

\begin{minipage}{\linewidth}
\begin{verbatim}
Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    26   79.746 MiB    0.000 MiB        1025       for k in range(n_n):
    27   79.746 MiB    0.000 MiB     1049600           for j in range(n_n):
    28   79.746 MiB    0.000 MiB     3145728               xr_o[k] += x_r[j] * \
    29   79.746 MiB    0.000 MiB     2097152                   math.cos(j * k * pi_2 / n_n) + x_i[j] * math.sin(j * k * pi_2 / n_n)
    30   79.746 MiB    0.301 MiB     3145728               xi_o[k] += -x_r[j] * \
    31   79.746 MiB    0.000 MiB     2097152                   math.sin(j * k * pi_2 / n_n) + x_i[j] * math.cos(j * k * pi_2 / n_n)
\end{verbatim}
\end{minipage}


\section{Exercise 6 - Experiment with the Python Debugger}

Advantages of using a debugger:
\begin{itemize}
    \item Debugging Code: Debuggers allow you to identify and fix bugs in your code by allowing you to step through your code and inspect the state of the program at any given point.
    \item Improving Code Understanding: Debuggers provide an interactive way to understand how your code is executing and can help you to understand how different parts of your code are interacting with each other.
    \item Time-Saving: Debuggers allow you to quickly find and fix bugs in your code, which can save you time compared to manual debugging techniques such as print statements.
    \item Debugging in Real-Time: Debuggers allow you to debug your code in real-time, giving you immediate feedback on what is happening in your code as it executes.
\end{itemize}

My personal struggles with using the pdb debugger were limited. 
Since I had a bit of experience using the command line interface with the gdb debugger, which had similar instructions.
Also the command:
\begin{lstlisting}[language=bash]
  $ pdb -p (pid of running process)
\end{lstlisting}
, to attach pdb to a running process, was one new thing I learned about these debuggers.



\section{Bonus}

Will be handed in later.

\end{document}
